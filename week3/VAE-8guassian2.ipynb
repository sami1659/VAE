{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "from ipdb import set_trace as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv=['']\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parser the arguments\n",
    "parser = argparse.ArgumentParser(description='VAE Example')\n",
    "parser.add_argument('--batch-size', type=int, default=2048, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=100, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# we use CPU for computation\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#K = 1024\n",
    "K = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(BATCH_SIZE):\n",
    "    #8 gaussians\n",
    "    while 1:\n",
    "        theta = (np.pi/4) * torch.randint(0, 8, (BATCH_SIZE,)).float().to(device)\n",
    "        centers = torch.stack((torch.cos(theta), torch.sin(theta)), dim = -1)\n",
    "        noise = torch.randn_like(centers) * 0.1\n",
    "        yield centers + noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = train_loader = data_gen(args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc0 = nn.Linear(2, K)\n",
    "        self.fc1 = nn.Linear(K, K)\n",
    "        self.fc21 = nn.Linear(K, K)\n",
    "        self.fc22 = nn.Linear(K, K)\n",
    "        self.fc3 = nn.Linear(K, K)\n",
    "        self.fc4 = nn.Linear(K, K)\n",
    "        self.fc5 = nn.Linear(K, 2)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.selu(self.fc1(F.selu(self.fc0(x))))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.fc5(F.selu(self.fc4(F.selu(self.fc3(z)))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 2))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose = True, threshold = 1E-2, eps=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Sequential and sample\n",
    "your comment for last code update:\n",
    "\"To simplify the definition of the layers in the VAE and the usage of the nonlinearities, you can use torch.nn.Sequential.\n",
    "* Aim to integrate the prior into your model in the sense that at a later stage we could easily replace the prior. \n",
    "Currently, for instance, you sample from the prior in __main__ to generate data. That should rather be part of the VAE as not each VAE we consider will probably use a zero-mean unit-variance Gaussian and it is then error-prone to do such sampling outside of the VAE.\"\n",
    "I try to define VAE with torch.nn.Sequential and define sample as a part of VAE.\n",
    "A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ordered dict of modules can also be passed in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encodinglayer1 = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size), nn.ReLU()\n",
    "        )\n",
    "        self.encodinglayer2_mean = nn.Sequential(nn.Linear(hidden_size, latent_size))\n",
    "        self.encodinglayer2_logvar = nn.Sequential(nn.Linear(hidden_size, latent_size))\n",
    "        self.decodinglayer = nn.Sequential(\n",
    "            nn.Linear(latent_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def sample(self, k):\n",
    "        sample = torch.randn(2048, K).to(device)\n",
    "        out = model.decode(sample).cpu().numpy()\n",
    "        recon = model(gt)[0].cpu().numpy()\n",
    "        return recon\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, input_size)\n",
    "        x = self.encodinglayer1(x)\n",
    "        log_var = self.encodinglayer2_logvar(x)\n",
    "        mean = self.encodinglayer2_mean(x)\n",
    "\n",
    "        z = self.sample(log_var, mean)\n",
    "        x = self.decodinglayer(z)\n",
    "\n",
    "        return x, mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    #BCE = F.binary_cross_entropy(recon_x, x.view(-1, 2), reduction='sum')\n",
    "    L2 = torch.mean((recon_x-x)**2)\n",
    "    \n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return L2 + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        if batch_idx > 100:\n",
    "            break #100 batches per epoch\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    train_loss /= 100\n",
    "    scheduler.step(train_loss)\n",
    "    print (train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test function\n",
    "for validation loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "           \n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "your comment for last update code:\n",
    "\"* Make the network smaller, e.g., use K=100.\n",
    "* Run more epochs, e.g., 1000, but implement an early stopping heuristic so that it can abort training when converged.\"\n",
    "\n",
    "Early stopping is a form of regularization used to avoid overfitting on the training dataset. Early stopping keeps track of the validation loss, if the loss stops decreasing for several epochs in a row the training stops. The EarlyStopping class in pytorchtool.py is used to create an object to keep track of the validation loss while training a PyTorch model. It will save a checkpoint of the model each time the validation loss decrease. We set the patience argument in the EarlyStopping class to how many epochs we want to wait after the last time the validation loss improved before breaking the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update\n",
    "# import EarlyStopping\n",
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update\n",
    "def train_model(model, batch_size, patience, n_epochs):\n",
    "    \n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, (data, target) in enumerate(train_loader, 1):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "             # calculate the loss\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target in valid_loader:\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            # calculate the loss\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(valid_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update\n",
    "#n_epochs = 100\n",
    "\n",
    "#train_loader, test_loader, valid_loader = create_datasets(batch_size)\n",
    "train_loader, test_loader, valid_loader = data_gen(args.batch_size)\n",
    "\n",
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 20\n",
    "\n",
    "model, train_loss, valid_loss = train_model(model, batch_size, patience, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next step\n",
    "your comment for last update code:\n",
    "\"What are next steps from perspective? From my perspective it would be great to run an experiment in which you\n",
    " report some form of error between the data generated from the VAE and the original data over different number of sizes of the latent space.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "\n",
    "    gt = next(train_loader)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(2048, K).to(device)\n",
    "        out = model.decode(sample).cpu().numpy()\n",
    "        recon = model(gt)[0].cpu().numpy()\n",
    "\n",
    "    rx,ry = recon[:,0], recon[:,1]\n",
    "        \n",
    "    gt = gt.cpu().numpy()\n",
    "    gx, gy = gt[:,0], gt[:,1]\n",
    "        \n",
    "    xs, ys = out[:,0], out[:,1]\n",
    "\n",
    "    plt.scatter(gx, gy, c = 'red', s=3)\n",
    "    plt.scatter(xs, ys, c = 'blue', s=3)\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.show()\n",
    "    \n",
    "    st()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
